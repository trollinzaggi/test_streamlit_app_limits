{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retry Logic Test Suite\n",
    "This notebook tests the retry decorators to ensure they work correctly for Azure OpenAI and Azure Search scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup - Import Retry Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "from functools import wraps\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import the retry logic from your optimization file\n",
    "# If this doesn't work, copy the retry functions from optimization_solutions.py\n",
    "try:\n",
    "    from optimization_solutions import (\n",
    "        retry_with_exponential_backoff,\n",
    "        retry_azure_openai,\n",
    "        retry_azure_search\n",
    "    )\n",
    "    print(\"Imported retry logic from optimization_solutions.py\")\n",
    "except ImportError:\n",
    "    print(\"Could not import from optimization_solutions.py\")\n",
    "    print(\"Copy the retry logic functions here if needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Test Metrics Tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestMetrics:\n",
    "    \"\"\"Track test execution metrics\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.attempts = 0\n",
    "        self.timestamps = []\n",
    "        self.errors = []\n",
    "        self.delays = []\n",
    "    \n",
    "    def record_attempt(self, error=None):\n",
    "        self.attempts += 1\n",
    "        current_time = datetime.now()\n",
    "        self.timestamps.append(current_time)\n",
    "        \n",
    "        if len(self.timestamps) > 1:\n",
    "            delay = (current_time - self.timestamps[-2]).total_seconds()\n",
    "            self.delays.append(delay)\n",
    "        \n",
    "        if error:\n",
    "            self.errors.append(str(error))\n",
    "    \n",
    "    def get_summary(self):\n",
    "        return {\n",
    "            'attempts': self.attempts,\n",
    "            'total_time': (self.timestamps[-1] - self.timestamps[0]).total_seconds() if len(self.timestamps) > 1 else 0,\n",
    "            'retry_delays': self.delays,\n",
    "            'errors': self.errors\n",
    "        }\n",
    "\n",
    "# Global metrics instance\n",
    "metrics = TestMetrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Test Scenario 1: Rate Limit (Should Retry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function that simulates rate limit errors\n",
    "@retry_azure_openai(max_retries=3)\n",
    "def test_rate_limit(fail_times=2):\n",
    "    \"\"\"Simulates Azure OpenAI rate limit errors\"\"\"\n",
    "    metrics.record_attempt()\n",
    "    \n",
    "    if metrics.attempts <= fail_times:\n",
    "        error = Exception(\"Error 429: Rate limit exceeded\")\n",
    "        metrics.record_attempt(error)\n",
    "        raise error\n",
    "    \n",
    "    return f\"Success after {metrics.attempts} attempts\"\n",
    "\n",
    "# Run the test\n",
    "print(\"TEST: Rate Limit Scenario\")\n",
    "print(\"Expected: Should retry with exponential backoff and succeed\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "metrics.reset()\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    result = test_rate_limit(fail_times=2)\n",
    "    print(f\"✓ SUCCESS: {result}\")\n",
    "    print(f\"Total time: {time.time() - start_time:.2f} seconds\")\n",
    "    print(f\"Retry delays: {metrics.delays}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ FAILED: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Test Scenario 2: Timeout (Should Retry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function that simulates timeout errors\n",
    "@retry_azure_search(max_retries=3)\n",
    "def test_timeout(fail_times=2):\n",
    "    \"\"\"Simulates Azure Search timeout errors\"\"\"\n",
    "    metrics.record_attempt()\n",
    "    \n",
    "    if metrics.attempts <= fail_times:\n",
    "        error = Exception(\"Request timeout after 30 seconds\")\n",
    "        metrics.record_attempt(error)\n",
    "        raise error\n",
    "    \n",
    "    return f\"Success after {metrics.attempts} attempts\"\n",
    "\n",
    "# Run the test\n",
    "print(\"TEST: Timeout Scenario\")\n",
    "print(\"Expected: Should retry with shorter delays (Azure Search)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "metrics.reset()\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    result = test_timeout(fail_times=2)\n",
    "    print(f\"✓ SUCCESS: {result}\")\n",
    "    print(f\"Total time: {time.time() - start_time:.2f} seconds\")\n",
    "    print(f\"Retry delays: {metrics.delays}\")\n",
    "except Exception as e:\n",
    "    print(f\"✗ FAILED: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Test Scenario 3: Non-Retriable Errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test unauthorized error (should NOT retry)\n",
    "@retry_azure_openai(max_retries=3)\n",
    "def test_unauthorized():\n",
    "    \"\"\"Simulates unauthorized error\"\"\"\n",
    "    metrics.record_attempt()\n",
    "    error = Exception(\"Error 401: Unauthorized - invalid API key\")\n",
    "    raise error\n",
    "\n",
    "# Test quota exceeded (should NOT retry)\n",
    "@retry_azure_openai(max_retries=3)\n",
    "def test_quota_exceeded():\n",
    "    \"\"\"Simulates quota exceeded error\"\"\"\n",
    "    metrics.record_attempt()\n",
    "    error = Exception(\"Error 403: Quota exceeded for this month\")\n",
    "    raise error\n",
    "\n",
    "# Run non-retriable tests\n",
    "for test_name, test_func in [(\"Unauthorized\", test_unauthorized), (\"Quota Exceeded\", test_quota_exceeded)]:\n",
    "    print(f\"\\nTEST: {test_name}\")\n",
    "    print(\"Expected: Should fail immediately without retry\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    metrics.reset()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        result = test_func()\n",
    "        print(f\"✗ UNEXPECTED SUCCESS\")\n",
    "    except Exception as e:\n",
    "        elapsed = time.time() - start_time\n",
    "        if metrics.attempts == 1 and elapsed < 1:\n",
    "            print(f\"✓ CORRECTLY FAILED: {str(e)}\")\n",
    "            print(f\"Attempts: {metrics.attempts} (no retries as expected)\")\n",
    "        else:\n",
    "            print(f\"✗ INCORRECTLY RETRIED: {metrics.attempts} attempts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test Scenario 4: Exhaust All Retries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test function that always fails\n",
    "@retry_azure_openai(max_retries=2)\n",
    "def test_always_fails():\n",
    "    \"\"\"Function that always fails to test retry exhaustion\"\"\"\n",
    "    metrics.record_attempt()\n",
    "    error = Exception(\"Service unavailable\")\n",
    "    raise error\n",
    "\n",
    "# Run the test\n",
    "print(\"TEST: Exhaust All Retries\")\n",
    "print(\"Expected: Should retry max_retries times then fail\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "metrics.reset()\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    result = test_always_fails()\n",
    "    print(f\"✗ UNEXPECTED SUCCESS\")\n",
    "except Exception as e:\n",
    "    print(f\"✓ CORRECTLY FAILED: {str(e)}\")\n",
    "    print(f\"Total attempts: {metrics.attempts}\")\n",
    "    print(f\"Total time: {time.time() - start_time:.2f} seconds\")\n",
    "    print(f\"Retry delays: {metrics.delays}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Visualize Retry Delays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with different retry counts to see exponential backoff\n",
    "test_results = []\n",
    "\n",
    "for fail_times in range(1, 4):\n",
    "    @retry_azure_openai(max_retries=3)\n",
    "    def test_backoff():\n",
    "        metrics.record_attempt()\n",
    "        if metrics.attempts <= fail_times:\n",
    "            raise Exception(\"Rate limit exceeded\")\n",
    "        return \"Success\"\n",
    "    \n",
    "    metrics.reset()\n",
    "    try:\n",
    "        test_backoff()\n",
    "        test_results.append({\n",
    "            'fail_times': fail_times,\n",
    "            'total_attempts': metrics.attempts,\n",
    "            'retry_delays': metrics.delays,\n",
    "            'total_time': sum(metrics.delays) if metrics.delays else 0\n",
    "        })\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# Visualize the exponential backoff\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Plot retry delays\n",
    "for result in test_results:\n",
    "    if result['retry_delays']:\n",
    "        attempts = list(range(1, len(result['retry_delays']) + 1))\n",
    "        ax1.plot(attempts, result['retry_delays'], marker='o', label=f\"{result['fail_times']} failures\")\n",
    "\n",
    "ax1.set_xlabel('Retry Attempt')\n",
    "ax1.set_ylabel('Delay (seconds)')\n",
    "ax1.set_title('Exponential Backoff Pattern')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot total time\n",
    "fail_counts = [r['fail_times'] for r in test_results]\n",
    "total_times = [r['total_time'] for r in test_results]\n",
    "ax2.bar(fail_counts, total_times, color='steelblue')\n",
    "ax2.set_xlabel('Number of Failures Before Success')\n",
    "ax2.set_ylabel('Total Retry Time (seconds)')\n",
    "ax2.set_title('Total Time Spent in Retries')\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nRetry Delay Analysis:\")\n",
    "for result in test_results:\n",
    "    print(f\"  {result['fail_times']} failures: {[f'{d:.2f}s' for d in result['retry_delays']]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Comparison: With vs Without Retry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a real-world scenario with intermittent failures\n",
    "import numpy as np\n",
    "\n",
    "def simulate_api_call(failure_rate=0.3):\n",
    "    \"\"\"Simulates an API call with given failure rate\"\"\"\n",
    "    if random.random() < failure_rate:\n",
    "        raise Exception(\"Rate limit exceeded\")\n",
    "    return \"Success\"\n",
    "\n",
    "# Test without retry\n",
    "successes_without_retry = 0\n",
    "failures_without_retry = 0\n",
    "total_calls = 100\n",
    "\n",
    "for _ in range(total_calls):\n",
    "    try:\n",
    "        simulate_api_call(failure_rate=0.3)\n",
    "        successes_without_retry += 1\n",
    "    except:\n",
    "        failures_without_retry += 1\n",
    "\n",
    "# Test with retry\n",
    "@retry_azure_openai(max_retries=3)\n",
    "def simulate_api_call_with_retry(failure_rate=0.3):\n",
    "    if random.random() < failure_rate:\n",
    "        raise Exception(\"Rate limit exceeded\")\n",
    "    return \"Success\"\n",
    "\n",
    "successes_with_retry = 0\n",
    "failures_with_retry = 0\n",
    "\n",
    "for _ in range(total_calls):\n",
    "    try:\n",
    "        simulate_api_call_with_retry(failure_rate=0.3)\n",
    "        successes_with_retry += 1\n",
    "    except:\n",
    "        failures_with_retry += 1\n",
    "\n",
    "# Display results\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': ['Success Rate', 'Failure Rate', 'Successful Calls', 'Failed Calls'],\n",
    "    'Without Retry': [\n",
    "        f\"{(successes_without_retry/total_calls)*100:.1f}%\",\n",
    "        f\"{(failures_without_retry/total_calls)*100:.1f}%\",\n",
    "        successes_without_retry,\n",
    "        failures_without_retry\n",
    "    ],\n",
    "    'With Retry': [\n",
    "        f\"{(successes_with_retry/total_calls)*100:.1f}%\",\n",
    "        f\"{(failures_with_retry/total_calls)*100:.1f}%\",\n",
    "        successes_with_retry,\n",
    "        failures_with_retry\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\nPerformance Comparison (100 API calls with 30% failure rate):\")\n",
    "display(comparison_df)\n",
    "\n",
    "# Visualize\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "x = np.arange(2)\n",
    "width = 0.35\n",
    "\n",
    "without_retry = [successes_without_retry, failures_without_retry]\n",
    "with_retry = [successes_with_retry, failures_with_retry]\n",
    "\n",
    "bars1 = ax.bar(x - width/2, without_retry, width, label='Without Retry', color=['green', 'red'])\n",
    "bars2 = ax.bar(x + width/2, with_retry, width, label='With Retry', color=['darkgreen', 'darkred'])\n",
    "\n",
    "ax.set_ylabel('Number of Calls')\n",
    "ax.set_title('Impact of Retry Logic on API Call Success Rate')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(['Successes', 'Failures'])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "improvement = ((successes_with_retry - successes_without_retry) / successes_without_retry) * 100\n",
    "print(f\"\\nImprovement with retry logic: {improvement:.1f}% more successful calls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Test Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run all tests and summarize\n",
    "test_suite = [\n",
    "    (\"Rate Limit Recovery\", test_rate_limit, {'fail_times': 2}, True),\n",
    "    (\"Timeout Recovery\", test_timeout, {'fail_times': 1}, True),\n",
    "    (\"Unauthorized No Retry\", test_unauthorized, {}, False),\n",
    "    (\"Quota Exceeded No Retry\", test_quota_exceeded, {}, False),\n",
    "    (\"Exhaust Retries\", test_always_fails, {}, False)\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"COMPLETE TEST SUITE RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for test_name, test_func, kwargs, should_succeed in test_suite:\n",
    "    metrics.reset()\n",
    "    \n",
    "    try:\n",
    "        result = test_func(**kwargs)\n",
    "        actual_result = \"Success\"\n",
    "        status = \"PASS\" if should_succeed else \"FAIL\"\n",
    "    except:\n",
    "        actual_result = \"Failed\"\n",
    "        status = \"PASS\" if not should_succeed else \"FAIL\"\n",
    "    \n",
    "    results.append({\n",
    "        'Test': test_name,\n",
    "        'Expected': \"Success\" if should_succeed else \"Failure\",\n",
    "        'Actual': actual_result,\n",
    "        'Status': status,\n",
    "        'Attempts': metrics.attempts\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "display(results_df)\n",
    "\n",
    "passed = len(results_df[results_df['Status'] == 'PASS'])\n",
    "total = len(results_df)\n",
    "\n",
    "print(f\"\\nTest Results: {passed}/{total} tests passed\")\n",
    "\n",
    "if passed == total:\n",
    "    print(\"✅ All tests passed! The retry logic is working correctly.\")\n",
    "else:\n",
    "    print(f\"⚠️ {total - passed} tests failed. Review the retry implementation.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}